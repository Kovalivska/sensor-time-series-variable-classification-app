# Datens√§tze f√ºr Variablenklassifikation

## √úberblick

Dieses Verzeichnis enth√§lt verschiedene Datens√§tze zur Erprobung und Validierung der entwickelten Methodik zur automatischen Variablenklassifikation. Die Datens√§tze repr√§sentieren typische industrielle Szenarien mit hoher Variablenkomplexit√§t.

##  Verf√ºgbare Datens√§tze

### **Reale Produktionsdaten** (`cnc_daten.csv`)
- **Herkunft**: Anonymisierte Daten aus realen industriellen Produktionsumgebungen
- **Typ**: CNC-Maschinendaten (Computer Numerical Control)
- **Charakteristika**: 
  - Echte Sensormesswerte aus der Produktion
  - Zeitreihen-basierte Maschinendaten
  - Verschiedene Messpunkte und Prozessparameter
  - Anonymisiert zur Wahrung der Vertraulichkeit

### **Synthetische Testdaten** (`large_dummy_data.csv`)
- **Herkunft**: K√ºnstlich generierte Sensordaten 
- **Zweck**: Methodenvalidierung und Stresstests
- **Charakteristika**:
  - Simulierte Daten verschiedener Sensortypen (Temperatur, Druck, Vibration, etc.)
  - Kontrollierte Variablenverteilung f√ºr Algorithmus-Testing
  - Hohe Anzahl an Variablen zur Skalierungstests
  - Bekannte Eigenschaften f√ºr Validierungszwecke

##  Warum Maschinendaten?

### Industrielle Realit√§t
In modernen Produktionsumgebungen entstehen t√§glich **Millionen von Datenpunkten** aus verschiedenen Quellen:

#### **Skalierung der Datenerfassung**
- **Sensordichte**: Moderne Produktionslinien verf√ºgen √ºber **hunderte bis tausende** von Sensoren
- **Messfrequenz**: Datenerfassung im **Millisekunden- bis Sekundenbereich**
- **Variablenvielfalt**: **1.000 bis 10.000+ Variablen** pro Produktionslinie sind keine Seltenheit
- **Datenvolumen**: **Terabytes** an Produktionsdaten pro Monat

#### **Zeitkritische Herausforderungen**
- **Manuelle Analyse unm√∂glich**: Bei tausenden Variablen ist eine h√§ndische Bewertung **zeitlich nicht praktikabel**
- **Expertenmangel**: Spezialisten f√ºr Datenanalyse sind **limitierte Ressourcen**
- **Produktionszyklen**: **Schnelle Entscheidungen** erforderlich f√ºr Prozessoptimierung
- **Kostendruck**: **Effiziente Datennutzung** direkt gesch√§ftskritisch

### üîç Problemstellung in der Praxis

#### **Variable √úberflutung (Data Overflow)**
```
Typische Produktionslinie:
‚îú‚îÄ‚îÄ Temperatur-Sensoren: 50-200 Variablen
‚îú‚îÄ‚îÄ Druck-Sensoren: 30-100 Variablen  
‚îú‚îÄ‚îÄ Vibrations-Sensoren: 100-500 Variablen
‚îú‚îÄ‚îÄ Durchfluss-Sensoren: 20-80 Variablen
‚îú‚îÄ‚îÄ Positions-Sensoren: 50-300 Variablen
‚îú‚îÄ‚îÄ Qualit√§ts-Parameter: 100-1000 Variablen
‚îî‚îÄ‚îÄ Status-Indikatoren: 200-2000 Variablen
    
GESAMT: 550-4.180 Variablen pro Linie!
```

#### **Manuelle Klassifikation: Zeitaufwand**
- **Einzelvariable**: 2-10 Minuten Analyse
- **1.000 Variablen**: 33-167 Arbeitsstunden  
- **5.000 Variablen**: 167-833 Arbeitsstunden (‚âà 4-20 Arbeitswochen!)
- **Wiederholung**: Bei neuen Produkten/Chargen erneut erforderlich

### Wertsch√∂pfung durch Automatisierung

#### **Geschwindigkeit**
- **Automatische Klassifikation**: Sekunden bis Minuten statt Wochen
- **Sofortige Empfehlungen**: Direkte Handlungsanweisungen
- **Skalierbarkeit**: Gleichzeitige Analyse multipler Produktionslinien

#### **Qualit√§t und Konsistenz**
- **Objektive Bewertung**: Eliminierung subjektiver Einsch√§tzungen
- **Reproduzierbarkeit**: Identische Ergebnisse bei wiederholter Analyse
- **Vollst√§ndigkeit**: Keine √úbersehung relevanter Variablen durch Erm√ºdung

#### **Wirtschaftlicher Nutzen**
- **Personalressourcen**: Experten konzentrieren sich auf **hochwertige Analyset√§tigkeiten**
- **Faster Time-to-Market**: **Beschleunigte Produkteinf√ºhrung** durch schnellere Datenanalyse
- **Qualit√§tsverbesserung**: **Fr√ºhere Erkennung** von Prozessabweichungen
- **Kostensenkung**: **Reduzierte Ausschussraten** durch bessere Prozess√ºberwachung

## Anwendungsszenarien

### **Neue Produktionslinien**
- **Schnelle Inbetriebnahme**: Automatische Identifikation relevanter √úberwachungsparameter
- **Prozessoptimierung**: Fr√ºhe Erkennung kritischer Stellschrauben
- **Qualit√§tssicherung**: Etablierung effektiver Monitoring-Strategien

### **Bestehende Anlagen**
- **Effizienzsteigerung**: Identifikation ungenutzter Datenquellen
- **Predictive Maintenance**: Fokussierung auf verschlei√ürelevante Parameter
- **Energieoptimierung**: Erkennung energierelevanter Variablen

### **Digitalisierungsprojekte**
- **IoT-Integration**: Bewertung neuer Sensordatenquellen
- **Big Data Analytics**: Vorselektion f√ºr ML-Modelle
- **Dashboard-Entwicklung**: Priorisierung von KPIs

## Datenqualit√§t und -integrit√§t

### **Realdaten-Anonymisierung**
- **Produktionsgeheimnisse**: Entfernung propriet√§rer Informationen
- **Personenbezug**: Keine R√ºckschl√ºsse auf Mitarbeiter oder Kunden
- **Wettbewerbsschutz**: Anonymisierung gesch√§ftskritischer Parameter
- **Compliance**: Einhaltung von Datenschutzbestimmungen

### **Synthetische Daten-Validierung**
- **Realit√§tsnahe Simulation**: Statistische Eigenschaften realer Sensordaten
- **Kontrollierte Testf√§lle**: Bekannte Ziel-Klassifikationen f√ºr Algorithmus-Validierung
- **Skalierungstests**: Gro√üe Variablenmengen f√ºr Leistungstests
- **Edge Cases**: Simulation seltener aber kritischer Datenmuster

## Technische Spezifikationen

### **Datenformat**
- **Format**: CSV (Comma-Separated Values)
- **Kodierung**: UTF-8 mit automatischer Erkennung
- **Trennzeichen**: Flexible Erkennung (Komma, Semikolon, Tab)
- **Zeitstempel**: ISO 8601 Format wo verf√ºgbar

### **Variablentypen**
- **Numerische Sensordaten**: Kontinuierliche Messwerte
- **Kategoriale Status**: Discrete Zustandsinformation
- **Zeitreihen**: Temporale Entwicklungen
- **Qualit√§tsparameter**: Spezifikations-Compliance-Metriken

## Nutzungshinweise

### **Dateigr√∂√üe und Performance**
- **Kleine Datens√§tze**: < 10 MB f√ºr schnelle Tests
- **Gro√üe Datens√§tze**: > 100 MB f√ºr Realtests
- **Speicheroptimierung**: Chunked Processing f√ºr gro√üe Files
- **Cache-Strategien**: Zwischenspeicherung berechneter Features

### **Datenschutz und Compliance**
- **Interne Nutzung**: Datens√§tze nur f√ºr Methodenentwicklung
- **Keine Weitergabe**: Schutz der zugrundeliegenden Produktionsdaten
- **Tempor√§re Verarbeitung**: Automatische Bereinigung nach Analyse
- **Audit-Trail**: Nachverfolgbarkeit der Datenverarbeitung

## Erste Schritte

1. **Datensatz ausw√§hlen**: Beginnen Sie mit `large_dummy_data.csv` f√ºr erste Tests
2. **App starten**: Verwenden Sie `streamlit run app7.1_final.py`
3. **Upload**: Laden Sie den gew√§hlten Datensatz hoch
4. **Analyse starten**: Klicken Sie auf "Start Analysis"
5. **Ergebnisse interpretieren**: Pr√ºfen Sie die generierten Empfehlungen

## Support und Weiterentwicklung

F√ºr R√ºckfragen zur Datennutzung oder Erweiterung der Testdatens√§tze erstellen Sie bitte ein GitHub Issue oder kontaktieren Sie das Entwicklungsteam.

---

*Diese Datens√§tze repr√§sentieren die Komplexit√§t realer industrieller Datenumgebungen und erm√∂glichen die realit√§tsnahe Erprobung automatisierter Klassifikationsmethoden.*
